{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named seaborn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-13f269b53b88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'matplotlib inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgeopandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named seaborn"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pysal as ps\n",
    "import mplleaflet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is our main data frame, containing both geometry and \n",
    "# attributes exported from CartoDB\n",
    "\n",
    "data = gpd.read_file('neighborhood_nta_census.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is only for the shapes that will be used by PySAL to \n",
    "# build the spatial weights matrix\n",
    "\n",
    "psGeom = ps.open('neighborhood_nta_census/neighborhood_nta_census.shp', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We are building the spatial weight matrix and using the \n",
    "# neighborhood names as IDs of the matrix. Noted that we\n",
    "# running a 'queen', shared vertices, neighborhood test.\n",
    "\n",
    "W = ps.buildContiguity(psGeom, criterion='queen', ids=data['ntaname'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's see how West Village is connected to other neighborhoods.\n",
    "# Notice the weights.\n",
    "\n",
    "W['West Village']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we would like to standardize all the weights. This can be \n",
    "# done by specifying 'R' as the matrix transformation. Then, let's\n",
    "# look again the neighbors of the West Village. All the weights\n",
    "# should add up to 1.\n",
    "\n",
    "W.transform = 'R'\n",
    "W['West Village']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next, we're going to perform a spatial autocorrelation on the\n",
    "# percent column. We first standardize the values by subtracting\n",
    "# the mean and divide by the standard deviation.\n",
    "\n",
    "Y = data['percent'].values\n",
    "Y = (Y-Y.mean())/Y.std() # <<<---- normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# and then compute the spatial lag for all neighborhoods based\n",
    "# on the spatial weight matrix. We also store this as a column\n",
    "# named 'w_percent' in the original table.\n",
    "\n",
    "sl = ps.lag_spatial(W, Y)\n",
    "data['w_percent'] = sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Execute the Moran's I calculation\n",
    "\n",
    "mi = ps.Moran(Y, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is the Moran's I value, that would tell us whether population\n",
    "# changes in New York are clustered, or not.\n",
    "\n",
    "mi.I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check the p-value of the calculation. This has to be < 0.05 for our\n",
    "# calculation to be statistically significant.\n",
    "\n",
    "mi.p_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It's time to look at the Moran Scatter Plot to inspet the results\n",
    "\n",
    "f, ax = plt.subplots(1, figsize=(10,10))\n",
    "sns.regplot(x='percent', y='w_percent', data=data)\n",
    "plt.axvline(0, c='k', alpha=0.5)\n",
    "plt.axhline(0, c='k', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### THE BELOW CODE SHOULD WORK FOR MOST INSTALLATION\n",
    "f, ax = plt.subplots(1, figsize=(10,10))\n",
    "data.plot(column='w_percent', scheme='QUANTILES', k=7, alpha=1, colormap='YlOrRd')\n",
    "#mplleaflet.display(fig=f, crs=data.crs)\n",
    "\n",
    "#### BUT IF NOT, PLEASE USE THE BELOW INSTEAD (and comment the previous blob)\n",
    "# data.plot(column='w_percent', scheme='QUANTILES', k=7, alpha=1.0, colormap='YlOrRd', figsize=(10,10))\n",
    "# mplleaflet.display(crs=data.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, let's look at local indicator. Overall, we see some\n",
    "# trends, but not so strong. Maybe a local indicator test\n",
    "# could help us see in details how things are correlated.\n",
    "# We run the Moran's LISA calculation provided by PySAL.\n",
    "\n",
    "lisa = ps.Moran_Local(Y, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's narrow down to those neighborhoods that are\n",
    "# statistically significant.\n",
    "\n",
    "S = lisa.p_sim < 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And which quadrants they belong to\n",
    "\n",
    "Q = lisa.q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Next, we'll turn those into a GeoDataFrame for visualization.\n",
    "\n",
    "records = map(lambda x: (data.iloc[x]['ntaname'], Q[x], data.geometry.iloc[x]),\n",
    "              [i for i,s in enumerate(S) if s])\n",
    "\n",
    "\n",
    "gdata = gpd.GeoDataFrame(records, columns=('ntaname', 'quadrant', 'geometry'))\n",
    "gdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# And plotting it with a basemap\n",
    "f, ax = plt.subplots(1, figsize=(10,10))\n",
    "gdata.plot(column='quadrant', scheme='QUANTILES', k=4, alpha=1.0, colormap='Blues')\n",
    "#mplleaflet.display(fig=f, crs=gdata.crs)\n",
    "\n",
    "##### Similar to the previous case, you can use the following if the above doesn't work\n",
    "# gdata.plot(column='quadrant', scheme='QUANTILES', k=4, alpha=1.0, colormap='Blues', figsize=(10,10))\n",
    "# mplleaflet.display(crs=gdata.crs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
